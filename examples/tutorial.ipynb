{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from mnn_torch.devices import load_SiOx_multistate\n",
    "from mnn_torch.models import MSNN, MCSNN\n",
    "from snntorch import surrogate\n",
    "from mnn_torch.effects import compute_PooleFrenkel_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9292, 0.4785, 0.2735],\n",
      "        [0.7004, 0.6785, 0.4075],\n",
      "        [0.5779, 0.8990, 0.7539],\n",
      "        [0.1442, 0.6784, 0.9076],\n",
      "        [0.3517, 0.0471, 0.5894]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4090'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test cuda availability\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load experimental data\n",
    "current_dir = os.getcwd()\n",
    "experimental_data = load_SiOx_multistate(\"../data/SiO_x-multistate-data.mat\")\n",
    "(G_off, G_on, R, c, d_epsilon) = compute_PooleFrenkel_parameters(experimental_data)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "num_inputs = 28 * 28\n",
    "num_hidden = 100\n",
    "num_outputs = 10\n",
    "num_steps = 25\n",
    "beta = 0.95\n",
    "data_path = \"../data\"\n",
    "lr = 5e-4\n",
    "\n",
    "# Memristive configuration\n",
    "PF_config = {\n",
    "    \"ideal\": False,\n",
    "    \"k_V\": 0.5,\n",
    "    \"G_off\": G_off,\n",
    "    \"G_on\": G_on,\n",
    "    \"R\": R,\n",
    "    \"c\": c,\n",
    "    \"d_epsilon\": d_epsilon,\n",
    "    \"disturb_conductance\": True,\n",
    "    \"disturb_mode\": \"fixed\",\n",
    "    \"disturbance_probability\": 0.1,\n",
    "    \"homeostasis_dropout\": True,\n",
    "    \"homeostasis_threshold\": 10,\n",
    "}\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0,), (1,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mnist_train = datasets.MNIST(\n",
    "    data_path, train=True, download=True, transform=transform\n",
    ")\n",
    "mnist_test = datasets.MNIST(\n",
    "    data_path, train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    mnist_train, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    mnist_test, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memristive configuration\n",
    "def train_model_with_dropout(homeostasis_dropout=True):\n",
    "    PF_config = {\n",
    "        \"ideal\": False,\n",
    "        \"k_V\": 0.5,\n",
    "        \"G_off\": G_off,\n",
    "        \"G_on\": G_on,\n",
    "        \"R\": R,\n",
    "        \"c\": c,\n",
    "        \"d_epsilon\": d_epsilon,\n",
    "        \"disturb_conductance\": True,\n",
    "        \"disturb_mode\": \"device\",\n",
    "        \"disturbance_probability\": 0.01,\n",
    "        \"homeostasis_dropout\": homeostasis_dropout,\n",
    "        \"homeostasis_threshold\": 10,\n",
    "    }\n",
    "\n",
    "    # Initialize network\n",
    "    net = MSNN(num_inputs, num_hidden, num_outputs, num_steps, beta, PF_config).to(device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    test_acc_hist = []\n",
    "\n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        for iter_counter, (data, targets) in enumerate(training_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            net.train()\n",
    "            spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "            loss_val = sum(loss(mem_rec[step], targets) for step in range(num_steps))\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            if iter_counter % 50 == 0:\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    test_data, test_targets = next(iter(validation_loader))\n",
    "                    test_data, test_targets = test_data.to(device), test_targets.to(device)\n",
    "\n",
    "                    test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "                    test_loss = sum(\n",
    "                        loss(test_mem[step], test_targets) for step in range(num_steps)\n",
    "                    )\n",
    "                    test_loss_hist.append(test_loss.item())\n",
    "\n",
    "                    # Compute accuracy\n",
    "                    _, idx = test_spk.sum(dim=0).max(1)\n",
    "                    acc = (idx == test_targets).float().mean().item()\n",
    "                    test_acc_hist.append(acc)\n",
    "\n",
    "                    print(\n",
    "                        f\"Epoch {epoch}, Iteration {iter_counter}\\n\"\n",
    "                        f\"Train Loss: {loss_val.item():.2f}, Test Loss: {test_loss.item():.2f}, \"\n",
    "                        f\"Test Accuracy: {acc * 100:.2f}%\"\n",
    "                    )\n",
    "\n",
    "    print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
    "    return loss_hist, test_loss_hist, test_acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0\n",
      "Train Loss: 70.11, Test Loss: 70.99, Test Accuracy: 12.50%\n",
      "Epoch 0, Iteration 50\n",
      "Train Loss: 69.35, Test Loss: 68.31, Test Accuracy: 4.69%\n",
      "Epoch 0, Iteration 100\n",
      "Train Loss: 68.93, Test Loss: 64.91, Test Accuracy: 4.69%\n",
      "Epoch 0, Iteration 150\n",
      "Train Loss: 70.19, Test Loss: 66.88, Test Accuracy: 6.25%\n",
      "Epoch 0, Iteration 200\n",
      "Train Loss: 64.52, Test Loss: 68.70, Test Accuracy: 9.38%\n",
      "Epoch 0, Iteration 250\n",
      "Train Loss: 61.91, Test Loss: 66.10, Test Accuracy: 10.94%\n",
      "Epoch 0, Iteration 300\n",
      "Train Loss: 70.96, Test Loss: 70.69, Test Accuracy: 7.81%\n",
      "Epoch 0, Iteration 350\n",
      "Train Loss: 66.84, Test Loss: 63.45, Test Accuracy: 9.38%\n",
      "Epoch 0, Iteration 400\n",
      "Train Loss: 68.34, Test Loss: 64.42, Test Accuracy: 3.12%\n",
      "Epoch 0, Iteration 450\n",
      "Train Loss: 67.05, Test Loss: 71.20, Test Accuracy: 9.38%\n",
      "Epoch 0, Iteration 500\n",
      "Train Loss: 65.80, Test Loss: 67.54, Test Accuracy: 6.25%\n",
      "Epoch 0, Iteration 550\n",
      "Train Loss: 67.61, Test Loss: 66.07, Test Accuracy: 6.25%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m test_acc_hist_no_dropout \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train with homeostasis dropout\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m loss_hist_dropout, test_loss_hist_dropout, test_acc_hist_dropout \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomeostasis_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Train without homeostasis dropout\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss_hist_no_dropout, test_loss_hist_no_dropout, test_acc_hist_no_dropout \u001b[38;5;241m=\u001b[39m train_model_with_dropout(homeostasis_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[37], line 35\u001b[0m, in \u001b[0;36mtrain_model_with_dropout\u001b[1;34m(homeostasis_dropout)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     34\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 35\u001b[0m spk_rec, mem_rec \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss(mem_rec[step], targets) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps))\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mnn_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mnn_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\git\\mnn-torch\\src\\mnn_torch\\models.py:87\u001b[0m, in \u001b[0;36mMSNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m spk2_rec, mem2_rec \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# First layer\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m     cur1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     spk1, mem1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif1(cur1, mem1)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# Collect spikes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mnn_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mnn_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\git\\mnn-torch\\src\\mnn_torch\\layers.py:73\u001b[0m, in \u001b[0;36mMemristiveLinearLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m weights_and_bias \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mt(), bias], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Compute the memristive outputs\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemristive_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_and_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\git\\mnn-torch\\src\\mnn_torch\\layers.py:111\u001b[0m, in \u001b[0;36mMemristiveLinearLayer.memristive_outputs\u001b[1;34m(self, x, weights_and_bias)\u001b[0m\n\u001b[0;32m    107\u001b[0m         G \u001b[38;5;241m=\u001b[39m disturb_conductance_fixed(\n\u001b[0;32m    108\u001b[0m             G, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mG_on, true_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisturbance_probability\n\u001b[0;32m    109\u001b[0m         )\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisturb_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 111\u001b[0m         G \u001b[38;5;241m=\u001b[39m \u001b[43mdisturb_conductance_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mG_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mG_off\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrue_probability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisturbance_probability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Compute current\u001b[39;00m\n\u001b[0;32m    119\u001b[0m I_ind \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(V, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(G, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\git\\mnn-torch\\src\\mnn_torch\\effects.py:59\u001b[0m, in \u001b[0;36mdisturb_conductance_device\u001b[1;34m(G, G_on, G_off, true_probability)\u001b[0m\n\u001b[0;32m     57\u001b[0m G_flat \u001b[38;5;241m=\u001b[39m G[\u001b[38;5;241m~\u001b[39mstuck_mask]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Non-stuck conductance values\u001b[39;00m\n\u001b[0;32m     58\u001b[0m kde \u001b[38;5;241m=\u001b[39m gaussian_kde(G_flat)\n\u001b[1;32m---> 59\u001b[0m kde_samples \u001b[38;5;241m=\u001b[39m \u001b[43mkde\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mG_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Apply truncated normal distribution to mitigate bias near zero\u001b[39;00m\n\u001b[0;32m     62\u001b[0m a, b \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m kde_samples\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m kde_samples\u001b[38;5;241m.\u001b[39mstd(), \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\mnn_torch\\lib\\site-packages\\scipy\\stats\\_kde.py:477\u001b[0m, in \u001b[0;36mgaussian_kde.resample\u001b[1;34m(self, size, seed)\u001b[0m\n\u001b[0;32m    473\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(seed)\n\u001b[0;32m    474\u001b[0m norm \u001b[38;5;241m=\u001b[39m transpose(random_state\u001b[38;5;241m.\u001b[39mmultivariate_normal(\n\u001b[0;32m    475\u001b[0m     zeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md,), \u001b[38;5;28mfloat\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance, size\u001b[38;5;241m=\u001b[39msize\n\u001b[0;32m    476\u001b[0m ))\n\u001b[1;32m--> 477\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m means \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[:, indices]\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m means \u001b[38;5;241m+\u001b[39m norm\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and collect data for both configurations\n",
    "loss_hist_dropout = []\n",
    "test_loss_hist_dropout = []\n",
    "test_acc_hist_dropout = []\n",
    "\n",
    "loss_hist_no_dropout = []\n",
    "test_loss_hist_no_dropout = []\n",
    "test_acc_hist_no_dropout = []\n",
    "\n",
    "# Train with homeostasis dropout\n",
    "loss_hist_dropout, test_loss_hist_dropout, test_acc_hist_dropout = train_model_with_dropout(homeostasis_dropout=True)\n",
    "\n",
    "# Train without homeostasis dropout\n",
    "loss_hist_no_dropout, test_loss_hist_no_dropout, test_acc_hist_no_dropout = train_model_with_dropout(homeostasis_dropout=False)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(test_loss_hist_dropout, label='With Homeostasis Dropout', color='blue')\n",
    "plt.plot(test_loss_hist_no_dropout, label='Without Homeostasis Dropout', color='red')\n",
    "plt.title('Test Loss Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_acc_hist_dropout, label='With Homeostasis Dropout', color='blue')\n",
    "plt.plot(test_acc_hist_no_dropout, label='Without Homeostasis Dropout', color='red')\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnn_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
