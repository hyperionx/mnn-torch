{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate, utils, backprop\n",
    "import snntorch.functional as SF\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "validation_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_indices = list(range(len(training_set)))\n",
    "np.random.shuffle(train_indices)\n",
    "train_sampler = SubsetRandomSampler(train_indices[:5000])\n",
    "\n",
    "val_indices = list(range(len(validation_set)))\n",
    "np.random.shuffle(val_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices[:1000])\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 6, 6, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "basic_model = BasicModel().to(device)\n",
    "optimizer = torch.optim.Adam(basic_model.parameters(), lr=5e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.tensor(np.random.rand(100,28,28), dtype=dtype)\n",
    "# y = torch.ones([100, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_hist = []\n",
    "# for epoch in range(100):\n",
    "#     iter_counter = 0\n",
    "\n",
    "#     # forward pass\n",
    "#     basic_model.train()\n",
    "#     spk_rec, mem_rec = basic_model(x)\n",
    "\n",
    "#     loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "#     for step in range(100):\n",
    "#         loss_val += loss_fn(mem_rec[step], y)\n",
    "\n",
    "#     # Gradient calculation + weight update\n",
    "#     optimizer.zero_grad()\n",
    "#     loss_val.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     loss_hist.append(loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward pass\n",
    "        basic_model.train()\n",
    "        spk_rec, mem_rec = basic_model(inputs)\n",
    "\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(25):\n",
    "            loss_val += loss_fn(mem_rec[step], labels)        \n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss_val.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print(f'  batch {i + 1} loss: {last_loss}')\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 58.69397712707519\n",
      "  batch 200 loss: 58.71328857421875\n",
      "  batch 300 loss: 58.44194541931152\n",
      "  batch 400 loss: 59.31764968872071\n",
      "  batch 500 loss: 58.43505596160889\n",
      "  batch 600 loss: 58.14370433807373\n",
      "  batch 700 loss: 58.208292198181155\n",
      "  batch 800 loss: 57.54457901000976\n",
      "  batch 900 loss: 57.59623413085937\n",
      "  batch 1000 loss: 57.57421119689941\n",
      "  batch 1100 loss: 57.57990997314453\n",
      "  batch 1200 loss: 57.52271625518799\n",
      "LOSS train 57.52271625518799 valid tensor([58.4092], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f'runs/fashion_trainer_{timestamp}')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    basic_model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    basic_model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        spk_rec, mem_rec = basic_model(vinputs)\n",
    "        vloss = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(25):\n",
    "            vloss += loss_fn(mem_rec[step], labels)  \n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(basic_model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data = next(iter(training_loader))\n",
    "inputs, labels = batch_data\n",
    "\n",
    "output, _ = basic_model(inputs)\n",
    "_, idx = output.sum(dim=0).max(1)\n",
    "acc = np.mean((labels == idx).detach().cpu().numpy())\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnn_torch.devices import load_SiOx_multistate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2656, 0.6610, 0.9424],\n",
      "        [0.6912, 0.2038, 0.6678],\n",
      "        [0.9582, 0.2312, 0.0995],\n",
      "        [0.6485, 0.9269, 0.7110],\n",
      "        [0.7278, 0.5288, 0.1134]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4090'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
